Tropical cyclone forecast model
A tropical cyclone forecast model is a computer program that uses meteorological data to forecast aspects of the future state of tropical cyclones. There are three types of models: statistical, dynamical, or combined statistical-dynamic.[1] Dynamical models utilize powerful supercomputers with sophisticated mathematical modeling software and meteorological data to calculate future weather conditions. Statistical models forecast the evolution of a tropical cyclone in a simpler manner, by extrapolating from historical datasets, and thus can be run quickly on platforms such as personal computers. Statistical-dynamical models use aspects of both types of forecasting. Four primary types of forecasts exist for tropical cyclones: track, intensity, storm surge, and rainfall. Dynamical models were not developed until the 1970s and the 1980s, with earlier efforts focused on the storm surge problem.
Track models did not show forecast skill when compared to statistical models until the 1980s. Statistical-dynamical models were used from the 1970s into the 1990s. Early models use data from previous model runs while late models produce output after the official hurricane forecast has been sent. The use of consensus, ensemble, and superensemble forecasts lowers errors more than any individual forecast model. Both consensus and superensemble forecasts can use the guidance of global and regional models runs to improve the performance more than any of their respective components. Techniques used at the Joint Typhoon Warning Center indicate that superensemble forecasts are a very powerful tool for track forecasting.


Statistical guidance[edit]
The first statistical guidance used by the National Hurricane Center was the Hurricane Analog Technique (HURRAN), which was available in 1969. It used the newly developed North Atlantic tropical cyclone database to find storms with similar tracks. It then shifted their tracks through the storm's current path, and used location, direction and speed of motion, and the date to find suitable analogs. The method did well with storms south of the 25th parallel which had not yet turned northward, but poorly with systems near or after recurvature.[2] Since 1972, the Climatology and Persistence (CLIPER) statistical model has been used to help generate tropical cyclone track forecasts. In the era of skillful dynamical forecasts, CLIPER is now being used as the baseline to show model and forecaster skill.[3] The Statistical Hurricane Intensity Forecast (SHIFOR) has been used since 1979 for tropical cyclone intensity forecasting. It uses climatology and persistence to predict future intensity, including the current Julian day, current cyclone intensity, the cyclone's intensity 12 hours ago, the storm's initial latitude and longitude, as well as its zonal (east-west) and meridional (north-south) components of motion.[2]
A series of statistical-dynamical models, which used regression equations based upon CLIPER output and the latest output from primitive equation models run at the National Meteorological Center, then National Centers for Environmental Prediction, were developed between the 1970s and 1990s and were named NHC73, NHC83, NHC90, NHC91, and NHC98.[1][4] Within the field of tropical cyclone track forecasting, despite the ever-improving dynamical model guidance which occurred with increased computational power, it was not until the decade of the 1980s when numerical weather prediction showed skill, and until the 1990s when it consistently outperformed statistical or simple dynamical models.[5] In 1994, a version of SHIFOR was created for the northwest Pacific Ocean for typhoon forecasting, known as the Statistical Typhoon Intensity Forecast (STIFOR), which used the 1971–1990 data for that region to develop intensity forecasts out to 72 hours into the future.[6]
In regards to intensity forecasting, the Statistical Hurricane Intensity Prediction Scheme (SHIPS) utilizes relationships between environmental conditions from the Global Forecast System (GFS) such as vertical wind shear and sea surface temperatures, climatology, and persistence (storm behavior) via multiple regression techniques to come up with an intensity forecast for systems in the northern Atlantic and northeastern Pacific oceans.[1] A similar model was developed for the northwest Pacific Ocean and Southern Hemisphere known as the Statistical Intensity Prediction System (STIPS), which accounts for land interactions through the input environmental conditions from the Navy Operational Global Prediction System (NOGAPS) model.[7] The version of SHIPS with an inland decay component is known as Decay SHIPS (DSHIPS). The Logistic Growth Equation Model (LGEM) uses the same input as SHIPS but within a simplified dynamical prediction system.[1] Within tropical cyclone rainfall forecasting, the Rainfall Climatology and Persistence (r-CLIPER) model was developed using microwave rainfall data from polar orbiting satellites over the ocean and first-order rainfall measurements from the land, to come up with a realistic rainfall distribution for tropical cyclones based on the National Hurricane Center's track forecast. It has been operational since 2004.[8] A statistical-parametric wind radii model has been developed for use at the National Hurricane Center and Joint Typhoon Warning Center which uses climatology and persistence to predict wind structure out to five days into the future.[2]
Dynamical guidance[edit]
During 1972, the first model to forecast storm surge along the continental shelf of the United States was developed, known as the Special Program to List the Amplitude of Surges from Hurricanes (SPLASH).[9] In 1978, the first hurricane-tracking model based on atmospheric dynamics – the movable fine-mesh (MFM) model – began operating.[10] The Quasi-Lagrangian Limited Area (QLM) model is a multi-level primitive equation model using a Cartesian grid and the Global Forecast System (GFS) for boundary conditions.[2] In the early 1980s, the assimilation of satellite-derived winds from water vapor, infrared, and visible satellite imagery was found to improve tropical cyclones track forecasting.[11] The Geophysical Fluid Dynamics Laboratory (GFDL) hurricane model was used for research purposes between 1973 and the mid-1980s. Once it was determined that it could show skill in hurricane prediction, a multi-year transition transformed the research model into an operational model which could be used by the National Weather Service for both track and intensity forecasting in 1995.[12] By 1985, the Sea Lake and Overland Surges from Hurricanes (SLOSH) Model had been developed for use in areas of the Gulf of Mexico and near the United States' East coast, which was more robust than the SPLASH model.[13]
The Beta Advection Model (BAM) has been used operationally since 1987 using steering winds averaged through the 850 hPa to 200 hPa layer and the Beta effect which causes a storm to drift northwest due to differences in the coriolis effect across the tropical cyclone.[14] The larger the cyclone, the larger the impact of the beta effect is likely to be.[15] Starting in 1990, three versions of the BAM were run operationally: the BAM shallow (BAMS) average winds in an 850 hPa to 700 hPa layer, the BAM Medium (BAMM) which uses average winds in an 850 hPa to 400 hPa layer, and the BAM Deep (BAMD) which is the same as the pre-1990 BAM.[4] For a weak hurricane without well-developed central thunderstorm activity, BAMS works well, because weak storms tend to be steered by low-level winds.[1] As the storm grows stronger and associated thunderstorm activity near its center gets deeper, BAMM and BAMD become more accurate, as these types of storms are steered more by the winds in the upper-level. If the forecast from the three versions is similar, then the forecaster can conclude that there is minimal uncertainty, but if the versions vary by a great deal, then the forecaster has less confidence in the track predicted due to the greater uncertainty.[16] Large differences between model predictions can also indicate wind shear in the atmosphere, which could affect the intensity forecast as well.[1]
Tested in 1989 and 1990, The Vic Ooyama Barotropic (VICBAR) model used a cubic-B spline representation of variables for the objective analysis of observations and solutions to the shallow-water prediction equations on nested domains, with the boundary conditions defined as the global forecast model.[17] It was implemented operationally as the Limited Area Sine Transform Barotropic (LBAR) model in 1992, using the GFS for boundary conditions.[2] By 1990, Australia had developed its own storm surge model which was able to be run in a few minutes on a personal computer.[18] The Japan Meteorological Agency (JMA) developed its own Typhoon Model (TYM) in 1994,[19] and in 1998, the agency began using its own dynamic storm surge model.[20]
The Hurricane Weather Research and Forecasting (HWRF) model is a specialized version of the Weather Research and Forecasting (WRF) model and is used to forecast the track and intensity of tropical cyclones. The model was developed by the National Oceanic and Atmospheric Administration (NOAA), the U.S. Naval Research Laboratory, the University of Rhode Island, and Florida State University.[21] It became operational in 2007.[22] Despite improvements in track forecasting, predictions of the intensity of a tropical cyclone based on numerical weather prediction continue to be a challenge, since statiscal methods continue to show higher skill over dynamical guidance.[23] Other than the specialized guidance, global guidance such as the GFS, Unified Model (UKMET), NOGAPS, Japanese Global Spectral Model (GSM), European Centre for Medium-Range Weather Forecasts model, France's Action de Recherche Petite Echelle Grande Echelle (ARPEGE) and Aire Limit´ee Adaptation Dynamique Initialisation (ALADIN) models, India's National Centre for Medium Range Weather Forecasting (NCMWRF) model, Korea's Global Data Assimilation and Prediction System (GDAPS) and Regional Data Assimilation and Prediction System (RDAPS) models, Hong Kong/China's Operational Regional Spectral Model (ORSM) model, and Canadian Global Environmental Multiscale Model (GEM) model are used for track and intensity purposes.[2]
Timeliness[edit]
Some models do not produce output quickly enough to be used for the forecast cycle immediately after the model starts running (including HWRF, GFDL, and FSSE). Most of the above track models (except CLIPER) require data from global weather models, such as the GFS, which produce output about four hours after the synoptic times of 0000, 0600, 1200, and 1800 Universal Coordinated Time (UTC). For half of their forecasts, the NHC issues forecasts only three hours after that time, so some "early" models — NHC90, BAM, and LBAR — are run using a 12-hour-old forecast for the current time. "Late" models, such as the GFS and GFDL, finish after the advisory has already been issued. These models are interpolated to the current storm position for use in the following forecast cycle — for example, GFDI, the interpolated version of the GFDL model.[1][24]
Consensus methods[edit]
Using a consensus of forecast models reduces forecast error.[25] Trackwise, the GUNA model is a consensus of the interpolated versions of the GFDL, UKMET with quality control applied to the cyclone tracker, United States Navy NOGAPS, and GFS models. The version of the GUNA corrected for model biases is known as the CGUN. The TCON consensus is the GUNA consensus plus the Hurricane WRF model. The version of the TCON corrected for model biases is known as the TCCN. A lagged average of the last two runs of the members within the TCON plus the ECMWF model is known as the TVCN consensus. The version of the TVCN corrected for model biases is the TVCC consensus.[1]
In early 2013, The NAVGEM replaced the NOGAPS as the Navy's primary operational global forecast model. For the 2013 season, and until model verification can occur, it is not being utilized in the development of any consensus forecasts.
For intensity, a combination of the LGEM, interpolated GFDL, interpolated HWRF, and DSHIPS models is known as the ICON consensus. The lagged average of the last two runs of models within the ICON consensus is called the IVCN consensus.[1] Across the northwest Pacific and Southern Hemisphere, a ten-member STIPS consensus is formed from the output of the NOGAPS, GFS, the Japanese GSM, the Coupled Ocean/Atmosphere Mesoscale Prediction System (COAMPS), the UKMET, the Japanese TYM, the GFDL with NOGAPS boundary conditions, the Air Force Weather Agency (AFWA) Model, the Australian Tropical Cyclone Local Area Prediction System, and the Weber Barotropic Model.[7]
Ensemble methods[edit]
No model is ever perfectly accurate because it is impossible to learn exactly everything about the atmosphere in a timely enough manner, and atmospheric measurements that are taken are not completely accurate.[26] The use of the ensemble method of forecasting, whether it be a multi-model ensemble, or numerous ensemble members based on the global model, helps define the uncertainty and further limit errors.[27][28]
The JMA has produced an 11-member ensemble forecast system for typhoons known as the Typhoon Ensemble Prediction System (TEPS) since February 2008, which is run out to 132 hours into the future. It uses a lower resolution version (with larger grid spacing) of its GSM, with ten perturbed members and one non-perturbed member. The system reduces errors by an average of 40 kilometres (25 mi) five days into the future when compared to its higher resolution GSM.[29]
The Florida State Super Ensemble (FSSE) is produced from a suite of models which then uses statistical regression equations developed over a training phase to reduce their biases, which produces forecasts better than the member models or their mean solution. It uses 11 global models, including five developed at Florida State University, the Unified Model, the GFS, the NOGAPS, the United States Navy NOGAPS, the Australian Bureau of Meteorology Research Centre (BMRC) model, and Canadian Recherche en Prévision Numérique (RPN) model. It shows significant skill in track, intensity, and rainfall predictions of tropical cyclones.[30]
The Systematic Approach Forecast Aid (SAFA) was developed by the Joint Typhoon Warning Center to create a selective consensus forecast which removed more erroneous forecasts at a 72‑hour time frame from consideration using the United States Navy NOGAPS model, the GFDL, the Japan Meteorological Agency's global and typhoon models, as well as the UKMET. All the models improved during SAFA's five-year history and removing erroneous forecasts proved difficult to do in operations.[31]
Sunspot theory[edit]
A 2010 report correlates low sunspot activity with high hurricane activity. Analyzing historical data, there was a 25% chance of at least one hurricane striking the continental United States during a peak sunspot year; a 64% chance during a low sunspot year. In June 2010, the hurricanes predictors in the US were not using this information.[32]
Hurricane forecast model accuracy[edit]
The accuracy of hurricane forecast models can vary significantly from storm to storm. For some storms the factors affecting the hurricane track are relatively straightforward, and the models are not only accurate but they produce similar forecasts, while for other storms the factors affecting the hurricane track are more complex and different models produce very different forecasts.[33]
See also[edit]
References[edit]
External links[edit]

